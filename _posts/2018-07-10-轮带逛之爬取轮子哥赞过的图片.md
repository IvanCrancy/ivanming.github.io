---
layout: post
title: 轮带逛之爬取轮子哥赞过的图片
date: 2018-07-10
excerpt: "此爬虫程序的唯一作用为：根据关键词定向下载轮子哥赞过的回答里面的的素（mei）材（nv）图片。"
tags: [Python, 技术向]
feature: https://ivancrancy.github.io/ivanming.github.io/assets/img/post_image/features/5.png
comments: false
---
# 跟着轮子哥有肉吃

此爬虫程序的唯一作用为：根据关键词定向下载轮子哥赞过的回答里面的的素（mei）材（nv）图片。

## 部分代码

```
from httpclient import HttpClient
import asyncio
import aiohttp
from bs4 import BeautifulSoup
import keywords
import config
import html
import os
import re

ZHIHU_URL = 'https://www.zhihu.com'
LOGIN_URL = ZHIHU_URL + '/login/email'
VCZH_URL = ZHIHU_URL + '/people/excited-vczh' # 此处可以替换成别的目标用户，在people/后面粘贴保存即可

class ZhihuClient():
    def __init__(self, client, email, phone_num, password):
        self._session = client
        self._client = HttpClient(client)
        self._email = email
        self._phone_num = phone_num
        self._password = password
        self._finish = False
        self._commenttime = '1970'
        self._imgurl = asyncio.Queue()

    async def _login(self):
        # r = requests.get(ZHIHU_URL, headers=Default_Header)
        # results = re.compile(r"\<input\stype=\"hidden\"\sname=\"_xsrf\"\svalue=\"(\S+)\"", re.DOTALL).findall(r.text)
        # self._xsrf = results[0]
        # print (self._xsrf)
        if self._email != '':
            data = {'email': self._email, 'password': self._password, 'remember_me': 'true'}
        else:
            data = {'phone_num': self._phone_num, 'password': self._password, 'remember_me': 'true'}
        print (data)
        dic = await self._client.post_json(LOGIN_URL, data=data)
        await self._client.get(ZHIHU_URL)
        self._xsrf = self._session.cookies['_xsrf'].value
        print (dic['r'])
        print (self._xsrf)

    async def crawl_voteup_answer(self):
        await self._login()

        feed_num = 20
        start = '0'
        api_url = VCZH_URL + '/activities'
        while feed_num == 20:
            data = {'_xsrf':self._xsrf, 'start':start}
            dic = await self._client.post_json(api_url, data=data)
            if dic == None:
                print ('获取更多状态网络错误')
                continue

            feed_num = dic['msg'][0]
            soup = BeautifulSoup(dic['msg'][1], 'html.parser')
            acts = soup.find_all('div', class_='zm-profile-section-item zm-item clearfix')
            start = acts[-1]['data-time'] if len(acts) > 0 else 0
            for act in acts:
                # 查看所有的赞，其他“回答、关注”忽略
                if act.attrs['data-type-detail'] != "member_voteup_answer":
                    continue
                # 获取评论的链接
                comment_div = act.find_all('div', class_='zm-item-answer')
                if comment_div == []:
                    continue
                comment_link = comment_div[0]['data-aid']
                comment_url = ZHIHU_URL + '/r/answers/' + comment_link + '/comments'
                # 根据评论里的关键字判断出是否有目标
                HIT = await self._analyze_comments(comment_url)
                await asyncio.sleep(config.comment_interval)
                if HIT == False:
                    continue
                # 获取回答
                answer = act.find_all('textarea', class_='content')
                if answer == []:
                    continue
                answer[0] = html.unescape(answer[0].get_text())
                # 从回答中找出图片链接
                soup2 = BeautifulSoup(answer[0], 'html.parser')
                img_urls = soup2.find_all('img')
                for img in img_urls:
                    await self._imgurl.put(img.attrs['src'])

            asyncio.sleep(config.more_interval)
            print ('more...')

        await self._imgurl.put('the end')


    async def _analyze_comments(self, url):
        dic = await self._client.get_json(url)
        if dic == None:
            print ('获取评论网络错误')
            return False

        data = dic['data']
        count = 0
        for comment in data:
            self._commenttime = comment['createdTime']
            for keyword in keywords.keywords:
                if comment['content'].find(keyword) != -1:
                    count += 1
        if count >= 3:
            return True
        return False

    async def download_image(self):
        if not os.path.exists('img'):
            os.makedirs('img')
        self._count = 1
        while True:
            url = await self._imgurl.get()
            count = self._count
            self._count += 1
            print ('正在下载第 %s 张图片。。。' % count)
            imgname = url.split('/')[-1]
            # 避免重复下载
            if os.path.exists('./img/' + imgname):
                print ('第 %s 张图片已经下载过，不重复下载。。。' % count)
                continue
            if url == 'the end':
                print ('下载完毕')
                self._finish = True
                break
            await self._client.downloadfile(url, './img/' + imgname)
            await asyncio.sleep(config.img_interval)


    async def monitor(self):
        count = 1
        more_interval = config.more_interval
        comment_interval = config.comment_interval
        img_interval = config.img_interval
        while True:
            if self._finish == True:
                break
            print ()
            print ('目前下载队列还有：%s 个。' % self._imgurl.qsize())
            print ('大概分析到的赞的时间：' + self._commenttime)
            print ()
            count += 1
            config.more_interval = more_interval
            config.comment_interval = comment_interval
            config.img_interval = img_interval
            # 约 200s 后停 10s，避免知乎反爬虫
            if count > 10:
                count = 1
                config.more_interval = 10
                config.comment_interval = 10
                print ('让爬虫休息一下。。。')

            await asyncio.sleep(20)

```

- 完整代码可以去[我的GitHub](https://github.com/IvanCrancy/followvczh)查看


## 服用说明

### 原理
定向抓取轮子哥（理论上你也可以在zhihuclient.py里面修改目标用户）赞过的所有带图片回答，然后按照预设的关键词筛选出目标图片，并保存到本地。

### 环境与依赖

* 本程序运行的python版本 >= Python 3.5
* aiohttp,；
* BeautifulSoup

### how to start

- 当前目录下新建 auth.py，按如下格式写入你的知乎账号。默认会优先使用邮箱登陆。

    email = 'xxx@xxx.xxx'
    phone_num = 'xxxxxxxxxxx'
    password = 'xxxxx'

- 在 config.py 中可以对爬取页面的速度作调整，建议间隔放宽一点避免触发知乎的反爬虫机制

- 在 keywords.py 中可以修改爬取的关键字。

- 然后运行

    python3 main.py
    
- 图片会自动存放在 img 目录下，此时请打开服用
